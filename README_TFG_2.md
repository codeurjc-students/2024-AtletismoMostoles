# üèÉ‚Äç‚ôÇÔ∏è TFG - Sistema Distribuido para Club de Atletismo

Este proyecto es la continuaci√≥n del TFG original basado en una aplicaci√≥n monol√≠tica. Se ha redise√±ado usando una arquitectura de microservicios para mejorar la escalabilidad, el mantenimiento y la especializaci√≥n de cada funcionalidad.

---

## üß© Microservicios del sistema

### üîπ Service1 - Frontend Gateway
- Rol: punto de entrada para el frontend Angular.
- Exposici√≥n REST.
- Se comunica por gRPC con el resto de servicios.

### üîπ Service2 - Resultados y PDFs
- Gesti√≥n de resultados deportivos por atleta y evento.
- Generaci√≥n de informes PDF simulados.
- Comunicaci√≥n v√≠a gRPC y RabbitMQ.

### üîπ Service3 - Eventos y Notificaciones
- Gesti√≥n de eventos.
- Emisi√≥n de notificaciones en tiempo real al frontend v√≠a WebSocket.
- Comunicaci√≥n v√≠a gRPC y RabbitMQ.

---

## üõ∞Ô∏è Comunicaci√≥n entre servicios

| Origen     | Destino     | Protocolo      | Prop√≥sito                         |
|------------|-------------|----------------|-----------------------------------|
| `Service1` | `Service2`  | gRPC           | Consultar y guardar resultados    |
| `Service1` | `Service3`  | gRPC           | Crear, listar o borrar eventos    |
| `Service1` | RabbitMQ    | WebSocket      | Mostrar notificaciones al usuario |
| `Service2` | RabbitMQ    | AMQP           | Gestionar generaci√≥n de PDFs      |
| `Service3` | RabbitMQ    | AMQP           | Publicar nuevas notificaciones    |

---

## üìÅ Estructura del repositorio

```
/TFG-Proyecto-Atletismo/
‚îÇ
‚îú‚îÄ‚îÄ frontend/                     # Cliente Angular
‚îú‚îÄ‚îÄ service1-backend/             # Puerta de entrada (REST + gRPC)
‚îú‚îÄ‚îÄ Service2-Results-PDF/         # Resultados + generaci√≥n de PDFs
‚îú‚îÄ‚îÄ Service3-Events/              # Gesti√≥n de eventos y notificaciones
‚îú‚îÄ‚îÄ shared-protos/                # Archivos .proto comunes para gRPC
‚îú‚îÄ‚îÄ docker/                       # Archivos Dockerfile para definicion de cada imagen
‚îú‚îÄ‚îÄ k8s/                          # Archivos de definici√≥n de los manifiestos kubernetes
‚îú‚îÄ‚îÄ deploy_files/                 # Archivos de despliegue de servicio en AKS sin IaC
‚îú‚îÄ‚îÄ infra/                        # Archivos necesario para el despliegue con IaC
...                               ...
‚îú‚îÄ‚îÄ README_TFG_2.md               # Este Archivo
...                               ...
```
---

## üß™ Tecnolog√≠as utilizadas

- Spring Boot 3.5.0
- Angular
- gRPC + Protobuf
- Spring Data JPA + MySQL
- Spring WebSocket
- Spring AMQP + RabbitMQ

---

## Gu√≠a de Despliegue ‚Äî Atletismo M√≥stoles (AKS + MySQL + Storage)

Esta gu√≠a explica **dos formas** de desplegar *toda la infraestructura* y la aplicaci√≥n en Azure:

- **A) Script ‚Äútodo en uno‚Äù**: `deploy_AKS.sh` ‚Äî ideal para demos/local.
- **B) Infraestructura como C√≥digo (IaC)** con **Bicep** ‚Äî m√°s declarativo y versionable.

---

## Requisitos

- **Azure CLI** (az) 2.55+
- **Bicep CLI** (v0.24+). Si usas `az`, ya trae soporte para Bicep en la mayor√≠a de versiones.
- **kubectl** 1.28+
- **Helm 3**
- **Docker** (para build/push de im√°genes)
- **Bash** (Linux/Mac/WSL2/Git Bash). En Windows, preferible **WSL2** o **PowerShell** + WSL.

Inicia sesi√≥n y elige suscripci√≥n:

```bash
az login
az account set --subscription "<TU_SUBSCRIPCION_UUID>"
```

---

## A) Despliegue con script `deploy_AKS.sh` (recomendado para demo)

El script prepara **todo**:
- Resource Group + AKS con autoscaling
- MySQL Flexible Server + BBDD
- Storage Account + contenedor
- **Ingress NGINX** con **IP p√∫blica est√°tica** en el **Resource Group de nodos**
- Reglas NSG para NodePorts (necesarias en AKS + Standard LB)
- **cert-manager** + *ClusterIssuer* (Let‚Äôs Encrypt)
- **RabbitMQ operator** y tu `RabbitmqCluster`
- Build & push de im√°genes Docker
- Aplicaci√≥n (frontend + service1 + service2 + service3) + Ingress con **nip.io**

### 1) Ajusta variables (opcional)
Edita `deploy_files/deploy_AKS.sh` si quieres cambiar:
- Regi√≥n, nombres, tama√±os de VM
- Docker Hub vs ACR (`USE_DOCKER_HUB` / `USE_ACR`)
- Emails/hostnames
- Rutas de manifiestos K8s

### 2) Docker login (si usas Docker Hub)
```bash
docker login
```

### 3) Ejecuta
```bash
bash deploy_files/deploy_AKS.sh
```

Al final ver√°s algo como:
```
Resource Group:     rg-tfg-weu
AKS:                aks-tfg-cluster
MySQL FQDN:         <xxxxx>.mysql.database.azure.com
Storage Account:    tfgpdfs (container: resultspdf)
Ingress LB IP:      98.71.203.209
Frontend Host:      frontend.98.71.203.209.nip.io
```

### 4) Verificaci√≥n r√°pida
```bash
kubectl get pods -A
kubectl get svc -A
kubectl get ingress -A
```

Prueba HTTP/HTTPS (mientras se emite el certificado puedes usar `-k`):
```bash
curl -I -H "Host: frontend.<LB_IP>.nip.io" http://<LB_IP>/
curl -vkI https://frontend.<LB_IP>.nip.io/
```

> **nip.io** resuelve `<algo>.<IP>.nip.io` directamente a esa `IP`. Evita tener que comprar/dominar DNS para la demo.

---

## B) Despliegue con IaC (Bicep)

El Bicep declarativo de `infra/` crea:
- **AKS** con autoscaling
- **MySQL Flexible Server** (modo laboratorio: *publicNetworkAccess Enabled* + *firewall allowAll*)
- **Storage Account** + **contenedor**

> La **configuraci√≥n de Ingress / IP est√°tica / cert-manager / app** se realiza con comandos post-provisi√≥n (abajo). Esto te da control fino sin ensuciar el Bicep.

### 1) Estructura sugerida del repo

```
infra/
  main.bicep
  parameters.dev.json
deploy_files/
  deploy_AKS.sh
k8s/
  frontend/
    deployment.yaml
    service.yaml
    ingress.yaml
    hpa.yaml
    pdb.yaml
  service1/ ... (cm, secret, deployment, service, hpa, pdb)
  service2/ ... (cm, secret, deployment, service, hpa, pdb)
  service3/ ... (cm, secret, deployment, service, hpa, pdb)
  rabbitmq/
    rabbitmq-cluster.yaml
docker/
  frontend/Dockerfile
  service1/Dockerfile
  service2/Dockerfile
  service3/Dockerfile
```

## 2) Par\u00e1metros (ejemplo `infra/parameters.dev.json`)

> **No** pongas contrase√±as reales; p√°salas por CLI (`--parameters`) o usa Key Vault.

```jsonc
{
  "$schema": "https://schema.management.azure.com/schemas/2019-04-01/deploymentParameters.json#",
  "contentVersion": "1.0.0.0",
  "parameters": {
    "location": { "value": "westeurope" },
    "projectName": { "value": "tfg" },
    "aksName": { "value": "aks-tfg-cluster" },
    "aksVmSize": { "value": "Standard_DS2_v2" },
    "aksNodeCount": { "value": 1 },
    "aksMinCount": { "value": 1 },
    "aksMaxCount": { "value": 3 },
    "createLogAnalytics": { "value": true },
    "lawName": { "value": "log-tfg" },
    "mysqlServerName": { "value": "mysql-tfg-server" },
    "mysqlAdminUser": { "value": "mysqladmin" },
    "mysqlAdminPassword": { "value": "<PON_AQUI_UNA_PASSWORD_SEGURA>" },
    "mysqlVersion": { "value": "8.0.21" },
    "mysqlSkuName": { "value": "Standard_B1ms" },
    "mysqlStorageGB": { "value": 32 },
    "mysqlDatabases": {
      "value": ["service1_db","service2_db","service3_db"]
    },
    "storageAccountName": { "value": "tfgpdfs" },
    "pdfContainerName": { "value": "resultspdf" }
  }
}
```

### 3) Despliega con Bicep

Crea el **Resource Group** y lanza la implementaci√≥n:

```bash
# 3.1 Grupo
az group create -n rg-tfg-weu -l westeurope

# 3.2 Implementaci√≥n (IAAC)
az deployment group create \
  -g rg-tfg-weu \
  -f infra/main.bicep \
  -p infra/parameters.dev.json
```

Obt√©n **outputs** √∫tiles (MySQL FQDN, Storage Connection String, etc.):
```bash
az deployment group show -g rg-tfg-weu -n <deploymentName> --query properties.outputs
```

### 4) Bootstrap K8s
Configurar el cl√∫ster y desplegar la app (Ingress NGINX + IP fija, NSG, cert-manager, RabbitMQ, manifiestos, etc.).

```bash
bash infra/bootstrap_k8s.sh
```

### 5) Verificar

```bash
kubectl get pods -A
kubectl get svc -A
kubectl get ingress -A

# HTTP deber√≠a devolver 308 (redirige a HTTPS)
curl -I -H "Host: frontend.${PUBIP}.nip.io" http://$PUBIP/

# HTTPS (usar -k si el cert a√∫n se est√° emitiendo)
curl -vkI https://frontend.${PUBIP}.nip.io/
```

---

## Soluci√≥n de problemas

- **`<pending>` en EXTERNAL-IP** (Service `ingress-nginx-controller`):  
  Asegura que la IP p√∫blica **existe en el Node RG** y que has anotado/forzado `loadBalancerIP` + `azure-pip-name` + `azure-load-balancer-resource-group`.
- **Timeouts desde fuera** (80/443):  
  Casi siempre es el **NSG** bloqueando **NodePorts**. Crea la regla `AzureLoadBalancer -> 30000‚Äì32767`.
- **Git Bash mete una ruta rara en la anotaci√≥n `...health-probe-request-path`**:  
  Ev√≠tala usando **probe TCP** (`azure-load-balancer-health-probe-protocol=tcp`).
- **`Service "default/frontend" does not have any active Endpoint`** en logs de Ingress:  
  El Service/Deployment del frontend a√∫n no expone endpoints (pods no listos). Revisa `kubectl get pods` y readiness/liveness.
- **Let‚Äôs Encrypt ‚Äúinvalid‚Äù**:  
  Borra `challenge`, `order` y el `Secret` del cert y re-aplica el Ingress (el script ya incluye un *retry* opcional).
- **Im√°genes no tiran / ImagePullBackOff**:  
  Verifica `docker push`, credenciales de ACR (si aplican) y nombres/tags en los Deployments.
- **MySQL (laboratorio)**:  
  El Bicep deja **publicNetworkAccess Enabled** y un firewall **allowAll**. √ösalo solo para pruebas. Para producci√≥n: Private Access + VNET integration.

---

## Limpieza (evitar costes)

```bash
# Borra el Resource Group (elimina AKS, MySQL, Storage, etc.)
az group delete -n rg-tfg-weu --yes --no-wait
```

> Si el **MC_*** (Node RG) quedase ‚Äúhu√©rfano‚Äù, elim√≠nalo igual desde Azure Portal o CLI.

---

## Notas finales

- Este repo mezcla un **script operativo** (`deploy_AKS.sh`) y **IaC con Bicep**: as√≠ tienes una ruta **r√°pida** y otra **declarativa**.
- El **script** automatiza tambi√©n piezas que normalmente se gestionan fuera del Bicep (IP est√°tica en Node RG, NSG de NodePorts, Helm releases, im√°genes, etc.).
- Para **producci√≥n**, considera:
    - AKS con **Azure CNI**, **WAF + AGIC** o NGINX en modo ‚Äúinternal + App Gateway/WAF‚Äù,
    - MySQL con **acceso privado**, copias, HA, SKU GP/MO,
    - secretos en **Key Vault** + CSI Driver.

